import asyncio
import csv
import io
import json
import logging
from datetime import datetime

import gspread
import requests
import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from google.auth.transport.requests import Request
from google.oauth2.service_account import Credentials
from gspread.exceptions import WorksheetNotFound, APIError

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

# C·∫•u h√¨nh h·ªá th·ªëng
GOOGLE_SHEETS_SCOPE = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]
# C·∫•u h√¨nh t·ª´ file c·∫•u h√¨nh (AI_config) c·ªßa b·∫°n
# V√≠ d·ª•:
import os
CUSTOM_SEARCH_API_KEY = os.getenv("GGSEARCH_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
MAX_CONCURRENT_REQUESTS = 10

semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

app = FastAPI(title="Webservice t√¨m ki·∫øm & ph√¢n t√≠ch")

#############################################
# C√°c h√†m h·ªó tr·ª£ (s·ª≠ d·ª•ng l·∫°i t·ª´ ƒëo·∫°n code g·ªëc)
#############################################

async def crawl_webpage(url: str) -> str:
    """Crawl webpage kh√¥ng ƒë·ªìng b·ªô v·ªõi logging chi ti·∫øt"""
    async with semaphore:
        try:
            # S·ª≠ d·ª•ng requests trong async kh√¥ng ph·∫£i l√† t·ªëi ∆∞u nh∆∞ng ƒë√¢y ch·ªâ l√† v√≠ d·ª•
            # B·∫°n c√≥ th·ªÉ thay b·∫±ng th∆∞ vi·ªán aiohttp ho·∫∑c AsyncWebCrawler c·ªßa b·∫°n n·∫øu c√≥.
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                content = response.text
                logging.info(f"üï∑Ô∏è Crawled successfully: {url} ({len(content)} chars)")
                # Gi·ªõi h·∫°n 20000 k√Ω t·ª± nh∆∞ trong code g·ªëc
                return content[:20000]
            else:
                logging.error(f"üö® Crawl failed (HTTP {response.status_code}): {url}")
                return ""
        except Exception as e:
            logging.error(f"üö® Crawl failed: {url} - {str(e)[:200]}")
            return ""

def init_services():
    """Kh·ªüi t·∫°o d·ªãch v·ª• v·ªõi logging x√°c th·ª±c"""
    try:
        creds = Credentials.from_service_account_file(
            'service-account.json', 
            scopes=GOOGLE_SHEETS_SCOPE
        )
        gc = gspread.authorize(creds)
        logging.info("üîë Google Sheets authentication successful")
        
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash-lite')
        logging.info("üß† Gemini API initialized")
        
        return gc, model
    except Exception as e:
        logging.critical(f"üî¥ Critical initialization error: {str(e)}")
        raise

def get_keywords(gc, sheet_url):
    """L·∫•y d·ªØ li·ªáu keywords (c·ªôt A), timerange (c·ªôt B) v√† num (c·ªôt C) t·ª´ Google Sheets v·ªõi logging chi ti·∫øt"""
    try:
        spreadsheet = gc.open_by_url(sheet_url)
        worksheet = spreadsheet.sheet1  # s·ª≠ d·ª•ng sheet ƒë·∫ßu ti√™n
        data = worksheet.get_all_values()
        
        if not data or len(data) < 2:
            logging.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ngo√†i ti√™u ƒë·ªÅ tr√™n sheet")
            return []
        
        entries = []
        # Gi·∫£ s·ª≠ h√†ng ƒë·∫ßu ti√™n l√† ti√™u ƒë·ªÅ n√™n d·ªØ li·ªáu b·∫Øt ƒë·∫ßu t·ª´ h√†ng th·ª© 2
        for idx, row in enumerate(data[1:], start=2):
            if len(row) < 3:
                logging.warning(f"H√†ng {idx} kh√¥ng ƒë·ªß d·ªØ li·ªáu: {row}")
                continue
            
            keyword = row[0].strip()
            timerange = row[1].strip()  # v√≠ d·ª•: 'd1', 'w1', 'm1'
            try:
                num = int(row[2].strip()) if row[2].strip() != "" else 3
            except ValueError:
                logging.warning(f"Gi√° tr·ªã kh√¥ng h·ª£p l·ªá t·∫°i h√†ng {idx} ·ªü c·ªôt num: {row[2]}")
                num = 3

            entries.append({
                "keyword": keyword,
                "timerange": timerange,
                "num": num
            })
        
        logging.info(f"üìñ ƒê√£ t√¨m th·∫•y {len(entries)} b·∫£n ghi tr√™n sheet")
        return entries

    except WorksheetNotFound:
        logging.error("üìÑ Worksheet kh√¥ng t·ªìn t·∫°i - ki·ªÉm tra t√™n sheet/tab")
        return []
    except APIError as e:
        logging.error(f"üîê Sheets API Error: {e.response.json()['error']['message']}")
        return []

def google_search(keyword, timerange=None, num=3):
    """T√¨m ki·∫øm Google v·ªõi logging request/response"""
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        'key': CUSTOM_SEARCH_API_KEY, 
        'cx': SEARCH_ENGINE_ID, 
        'q': keyword, 
        'num': num
    }
    if timerange:
        params['dateRestrict'] = timerange
    try:
        logging.debug(f"üîé Searching for: {keyword} v·ªõi timerange={timerange} v√† num={num}")
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code != 200:
            logging.error(f"‚ö° Search API Error {response.status_code}: {response.text[:200]}")
            return []
            
        results = response.json().get('items', [])
        logging.info(f"üåê Found {len(results)} results for: {keyword}")
        return [item['link'] for item in results]
        
    except Exception as e:
        logging.error(f"üåê Search failed for {keyword}: {str(e)[:200]}")
        return []
    
def extract_json_from_response(gemini_response):
    start_index = gemini_response.find('{')
    end_index = gemini_response.rfind('}') + 1
    if start_index != -1 and end_index != -1:
        json_text = gemini_response[start_index:end_index]
        try:
            response_data = json.loads(json_text)
            return response_data
        except json.JSONDecodeError:
            return "ƒê√£ x·∫£y ra l·ªói khi ph√¢n t√≠ch ph·∫£n h·ªìi t·ª´ Gemini."
    else:
        return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu JSON trong ph·∫£n h·ªìi."
        
def analyze_content(content, model):
    """Ph√¢n t√≠ch n·ªôi dung v·ªõi logging ƒë·∫ßy ƒë·ªß"""
    prompt = f"""[Y√äU C·∫¶U B·∫ÆT BU·ªòC] 
    1. Tr·∫£ v·ªÅ K·∫æT QU·∫¢ DUY NH·∫§T d∆∞·ªõi d·∫°ng JSON h·ª£p l·ªá
    2. Kh√¥ng th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch hay markdown n√†o
    3. ƒê·∫£m b·∫£o ƒë√∫ng c·∫•u tr√∫c

    PH√ÇN T√çCH TIN T·ª®C:
    {content}

    K·∫æT QU·∫¢ PH·∫¢I THEO M·∫™U:
    {{
        "sentiment": "positive|neutral|negative",
        "impact": "high|medium|low",
        "reason": "l√Ω do ng·∫Øn g·ªçn d∆∞·ªõi 20 t·ª´"
    }}"""
    try:
        response = model.generate_content(prompt)
        analysis = extract_json_from_response(response.text)
        
        if not all(key in analysis for key in ['sentiment','impact','reason']):
            raise ValueError("Invalid analysis format")
            
        logging.debug(f"üìä Analysis result: {json.dumps(analysis, indent=2)}")
        return analysis
        
    except json.JSONDecodeError:
        logging.error("üî† Failed to parse Gemini response as JSON")
        raise
    except ValueError as ve:
        logging.error(f"üìâ Invalid analysis format: {str(ve)}")
        raise
    except Exception as e:
        logging.error(f"üß† Analysis failed: {str(e)[:200]}")
        raise

def save_results(gc, sheet_url, data):
    """L∆∞u k·∫øt qu·∫£ v·ªõi logging chi ti·∫øt v√†o Google Sheets"""
    if not data:
        logging.warning("üì≠ No data to save")
        return

    try:
        spreadsheet = gc.open_by_url(sheet_url)
        try:
            sheet = spreadsheet.worksheet('Results')
            logging.info("üìä Found existing Results worksheet")
        except WorksheetNotFound:
            sheet = spreadsheet.add_worksheet(title='Results', rows=1000, cols=20)
            sheet.append_row(['Keyword', 'URL', 'ƒê√°nh gi√°', 'T√°c ƒë·ªông', 'L√Ω do', 'Ng√†y ph√¢n t√≠ch'])
            logging.info("üìÑ Created new Results worksheet")
        
        logging.debug(f"üíæ Saving {len(data)} rows to sheet")
        sheet.append_rows(data)
        logging.info(f"‚úÖ Successfully saved {len(data)} records")
        
    except APIError as e:
        logging.error(f"üíæ Save failed - API Error: {e.response.json()['error']['message']}")
    except Exception as e:
        logging.error(f"üíæ Save failed: {str(e)[:200]}")

async def process_keyword(gc, model, entry, output_sheet_url):
    """X·ª≠ l√Ω t·ª´ng entry ch·ª©a keyword, timerange v√† num v·ªõi logging chi ti·∫øt"""
    keyword = entry.get("keyword")
    timerange = entry.get("timerange")
    num = entry.get("num", 3)
    
    logging.info(f"üî® Starting processing for: {keyword} (timerange={timerange}, num={num})")
    results = []
    
    try:
        urls = google_search(keyword, timerange, num)
        if not urls:
            logging.warning(f"üåê No URLs found for: {keyword}")
            return []
            
        logging.info(f"üåç Crawling {len(urls)} URLs for: {keyword}")
        # S·ª≠ d·ª•ng asyncio.gather ƒë·ªÉ crawl c√°c URL
        contents = await asyncio.gather(*[crawl_webpage(url) for url in urls])
        
        success_count = 0
        for url, content in zip(urls, contents):
            if not content:
                logging.warning(f"‚ö†Ô∏è Empty content for: {url}")
                continue
                
            try:
                analysis = analyze_content(content, model)
                results.append([
                    keyword,
                    url,
                    analysis.get('sentiment', 'N/A'),
                    analysis.get('impact', 'N/A'),
                    analysis.get('reason', 'N/A'),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                ])
                success_count += 1
            except Exception:
                logging.error(f"üîß Failed to analyze: {url}")
                
        logging.info(f"üìà Processed {success_count}/{len(urls)} URLs successfully for: {keyword}")
        return results
        
    except Exception as e:
        logging.error(f"üî• Failed to process keyword {keyword}: {str(e)[:200]}")
        return []

#############################################
# ENDPOINT 1: X·ª≠ l√Ω d·ª±a tr√™n Google Sheets
#############################################

@app.get("/process_sheet")
async def process_sheet(sheet_url: str):
    """
    Endpoint nh·∫≠n v√†o URL c·ªßa Google Sheets ch·ª©a d·ªØ li·ªáu ƒë·∫ßu v√†o (keyword, timerange, num)
    v√† c≈©ng l√† n∆°i l∆∞u k·∫øt qu·∫£ ƒë·∫ßu ra.
    """
    try:
        gc, model = init_services()
    except Exception as e:
        raise HTTPException(status_code=500, detail="Initialization error: " + str(e))
        
    entries = get_keywords(gc, sheet_url)
    if not entries:
        raise HTTPException(status_code=400, detail="Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu h·ª£p l·ªá tr√™n sheet.")
    
    logging.info(f"üöÄ Starting processing for {len(entries)} keywords from sheet")
    tasks = [process_keyword(gc, model, entry, sheet_url) for entry in entries]
    all_results = await asyncio.gather(*tasks)
    total_records = sum(len(batch) for batch in all_results if batch)
    
    # L∆∞u k·∫øt qu·∫£ v√†o Google Sheets
    for result_batch in all_results:
        if result_batch:
            save_results(gc, sheet_url, result_batch)
    
    return JSONResponse(content={
        "message": "Processing completed",
        "total_keywords": len(entries),
        "total_records": total_records
    })

#############################################
# ENDPOINT 2: X·ª≠ l√Ω theo JSON ƒë·∫ßu v√†o v√† tr·∫£ v·ªÅ CSV
#############################################

from pydantic import BaseModel

class SearchCriteria(BaseModel):
    keyword: str
    timerange: str = None  # v√≠ d·ª•: 'd1', 'w1', 'm1'
    num: int = 3

@app.post("/search")
async def search_and_download(criteria: SearchCriteria):
    """
    Endpoint nh·∫≠n JSON ch·ª©a keyword, timerange v√† num.
    Th·ª±c hi·ªán t√¨m ki·∫øm, crawl, ph√¢n t√≠ch v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ·ªü d·∫°ng file CSV.
    """
    # Kh·ªüi t·∫°o Gemini model (kh√¥ng c·∫ßn Google Sheets cho endpoint n√†y)
    try:
        # Ta ch·ªâ c·∫ßn model cho ph√¢n t√≠ch n·ªôi dung
        _, model = init_services()
    except Exception as e:
        raise HTTPException(status_code=500, detail="Initialization error: " + str(e))
    
    keyword = criteria.keyword
    timerange = criteria.timerange
    num = criteria.num
    
    logging.info(f"üî® Processing search for: {keyword} (timerange={timerange}, num={num})")
    
    try:
        urls = google_search(keyword, timerange, num)
        if not urls:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t√¨m ki·∫øm")
        
        contents = await asyncio.gather(*[crawl_webpage(url) for url in urls])
        results = []
        for url, content in zip(urls, contents):
            if not content:
                continue
            try:
                analysis = analyze_content(content, model)
                results.append([
                    keyword,
                    url,
                    analysis.get('sentiment', 'N/A'),
                    analysis.get('impact', 'N/A'),
                    analysis.get('reason', 'N/A'),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                ])
            except Exception:
                continue
        
        if not results:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ ph√¢n t√≠ch n·ªôi dung ƒë∆∞·ª£c")
        
        # T·∫°o file CSV t·ª´ k·∫øt qu·∫£
        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerow(['Keyword', 'URL', 'ƒê√°nh gi√°', 'T√°c ƒë·ªông', 'L√Ω do', 'Ng√†y ph√¢n t√≠ch'])
        writer.writerows(results)
        output.seek(0)
        
        filename = f"search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        headers = {
            'Content-Disposition': f'attachment; filename="{filename}"'
        }
        
        return StreamingResponse(output, media_type="text/csv", headers=headers)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail="Processing error: " + str(e))
